{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97da895-8c36-4246-b2f6-f757fa63fe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 21 16:34:58 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   39C    P8    28W / 300W |      3MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:05:00.0 Off |                  Off |\n",
      "| 58%   82C    P2   267W / 300W |  34105MiB / 48685MiB |     79%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:06:00.0 Off |                  Off |\n",
      "| 30%   32C    P8    25W / 300W |   9640MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:08:00.0 Off |                  Off |\n",
      "| 30%   32C    P8    21W / 300W |  15042MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000    Off  | 00000000:0E:00.0 Off |                  Off |\n",
      "| 30%   33C    P8    22W / 300W |      0MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000    Off  | 00000000:0F:00.0 Off |                  Off |\n",
      "| 30%   33C    P8    25W / 300W |  26153MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca20e034-6024-43b5-a29f-275e69b242b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(5)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f69c091-0d21-493c-9957-35f328fae708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q bitsandbytes\n",
    "# !pip install -q transformers \n",
    "# !pip install -q peft\n",
    "# !pip install -q accelerate\n",
    "# !pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312da2b5-1da0-4357-99ec-99b9239cf7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07105266-8f81-45ad-85f1-e6d77f23c4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831a0f48e7974ccab88cdd3723a89006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/753 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e6f7f83a16740638758ff67d971f9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/870k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a180826f3602494cb33f9270ff9a86ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/529k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbc3c68b04d447099c5fc4e76c04b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acb6b7fb1e44e96af40159caad2eb96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/95.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bbc879497b41a8a7db0fadecee4252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/232 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7741b8ebe3eb4b48b139eabcd66a9add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c186b87d49d4fed95fd07056a50bfcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/5.76G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a642791ef6734cb4a984a5717537a9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/144 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(50304, 2048, padding_idx=50258)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"42dot/42dot_LLM-SFT-1.3B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"42dot/42dot_LLM-SFT-1.3B\").to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf8ef65-ece9-4508-a4f8-e06b2c2b1a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자 : 자라나는 식물은 자라고 있다.\n",
      "전 : 전 세계적으로 자라는 식물은 약 10억 종에 달한다.\n",
      "거 : 거미, 벌, 나비, 새 등 다양한 동물들도 자란다.\n"
     ]
    }
   ],
   "source": [
    "question = \"간단한 문장을 제시하라.\"\n",
    "word = \"자전거\"\n",
    "prefix_prompt = f'### 입력: {question}\\n\\n### 답변:'\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for idx in range(len(word)):\n",
    "    char = word[idx]\n",
    "    \n",
    "    # encode context the generation is conditioned on\n",
    "    if len(outputs)==0:\n",
    "        input_sentence = f'{prefix_prompt}{\" \".join(outputs).strip()} {char}'\n",
    "    else:\n",
    "        input_sentence = f'{prefix_prompt} {\" \".join(outputs).strip()} {char}'\n",
    "    input_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)\n",
    "\n",
    "    \n",
    "    if idx == 0:\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_length=len(input_ids[0]) + 10,\n",
    "            early_stopping=True,\n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=1.25,\n",
    "        )\n",
    "    \n",
    "        \n",
    "    elif idx != len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            # max_length=len(input_ids[0]) + 10,\n",
    "            early_stopping=True,\n",
    "            # num_beams=10, \n",
    "            # no_repeat_ngram_size=2, \n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=0.5,\n",
    "            # top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "        )\n",
    "    \n",
    "    elif idx == len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            # max_length=len(input_ids[0]) + 10,\n",
    "            early_stopping=True,\n",
    "            # num_beams=10, \n",
    "            # no_repeat_ngram_size=2, \n",
    "            do_sample=False, # 샘플링 전략 사용\n",
    "            # top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "            \n",
    "            \n",
    "            # input_ids = input_ids, \n",
    "            # max_length=len(input_ids[0]) + 50,\n",
    "            # early_stopping=True,\n",
    "            # num_beams=10, \n",
    "            no_repeat_ngram_size=2, \n",
    "            # do_sample=True, # 샘플링 전략 사용\n",
    "            # top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(beam_output[0], skip_special_tokens=True)[len(input_sentence)-1:].split('#')[0].split('. ')[0].strip() # .split('\\n')[0]\n",
    "    outputs.append(output)\n",
    "\n",
    "    print(f\"{char} : {outputs[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2504b139-bab1-4975-8c33-b25beaae153d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인 : 인하대 학생들이여,\n",
      "하 : 하하하!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.25` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대 : 대동소이하다.\n"
     ]
    }
   ],
   "source": [
    "word = \"인하대\"\n",
    "question = f\"{word}에 대한 재미있고 간단한 문장을 말씀드리겠습니다.\"\n",
    "prefix_prompt = f'{question} '\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for idx in range(len(word)):\n",
    "    char = word[idx]\n",
    "    \n",
    "    # encode context the generation is conditioned on\n",
    "    if len(outputs)==0:\n",
    "        input_sentence = f'{prefix_prompt}{\" \".join(outputs).strip()} {char}'\n",
    "    else:\n",
    "        input_sentence = f'{prefix_prompt} {\" \".join(outputs).strip()} {char}'\n",
    "    input_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)\n",
    "\n",
    "    \n",
    "    if idx == 0:\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            early_stopping=True,\n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=0.5,\n",
    "        )\n",
    "    \n",
    "        \n",
    "    elif idx != len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2, \n",
    "            do_sample=False, # 샘플링 전략 사용\n",
    "            temperature=0.25,\n",
    "            # top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "        )\n",
    "    \n",
    "    elif idx == len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            # max_new_tokens = 100,\n",
    "            max_length=len(input_ids[0]) + 100,\n",
    "            early_stopping=True,\n",
    "            num_beams=10, \n",
    "            no_repeat_ngram_size=2, \n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=1,\n",
    "            top_p=0.90, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "            \n",
    "        )\n",
    "\n",
    "        \n",
    "    output = tokenizer.decode(beam_output[0], skip_special_tokens=True)[len(input_sentence)-1:].split('\\n')[0].split('#')[0]\n",
    "    if '.' in output:\n",
    "        output = output.split('.')[0] + '.'\n",
    "    if ',' in output:\n",
    "        output = output.split(',')[0] + ','\n",
    "\n",
    "        \n",
    "    outputs.append(output)\n",
    "\n",
    "    print(f\"{char} : {outputs[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ec8204c-d5a5-435d-8b59-bf0d41d628ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인 : 인하대 근처에 있는 카페에서 친구와 함께 산책을 하며 커피를 마시는데,\n",
      "하 : 하얀 눈이 내리는 날이었다.\n",
      "대 : 대자연의 아름다움을 만끽할 수 있었다.\n"
     ]
    }
   ],
   "source": [
    "word = \"인하대\"\n",
    "question = f\"{word}에 대한 재미있고 간단한 문장을 말씀드리겠습니다.\"\n",
    "prefix_prompt = f'{question} '\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for idx in range(len(word)):\n",
    "    char = word[idx]\n",
    "    \n",
    "    # encode context the generation is conditioned on\n",
    "    if len(outputs)==0:\n",
    "        input_sentence = f'{prefix_prompt}{\" \".join(outputs).strip()} {char}'\n",
    "    else:\n",
    "        input_sentence = f'{prefix_prompt} {\" \".join(outputs).strip()} {char}'\n",
    "    input_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)\n",
    "\n",
    "    \n",
    "    if idx == 0:\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            early_stopping=True,\n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=0.5,\n",
    "        )\n",
    "    \n",
    "        \n",
    "    elif idx != len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2, \n",
    "            do_sample=False, # 샘플링 전략 사용\n",
    "            temperature=0.25,\n",
    "            # top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "        )\n",
    "    \n",
    "    elif idx == len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            # max_new_tokens = 100,\n",
    "            max_length=len(input_ids[0]) + 100,\n",
    "            early_stopping=True,\n",
    "            num_beams=10, \n",
    "            no_repeat_ngram_size=2, \n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=1,\n",
    "            top_p=0.90, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "            \n",
    "        )\n",
    "\n",
    "        \n",
    "    output = tokenizer.decode(beam_output[0], skip_special_tokens=True)[len(input_sentence)-1:].split('\\n')[0].split('#')[0]\n",
    "    if '.' in output:\n",
    "        output = output.split('.')[0] + '.'\n",
    "    if ',' in output:\n",
    "        output = output.split(',')[0] + ','\n",
    "\n",
    "        \n",
    "    outputs.append(output)\n",
    "\n",
    "    print(f\"{char} : {outputs[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d5e00c-fafc-42c2-bd0a-6ebdfcfca925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 : 이채연은 귀여운 외모와 밝은 성격으로 많은 사람들의 사랑을 받았습니다..\n",
      "채 : 채연이는 항상 웃는 얼굴로 사람들을 행복하게 만들었습니다..\n",
      "은 : 은은한 미소와 상냥한 말투로 사람들의 마음을 편안하게 해주었습니다..\n"
     ]
    }
   ],
   "source": [
    "word = \"이채은\"\n",
    "question = f\"{word}에 대한 재미있고 간단한 문장을 말씀드리겠습니다.\"\n",
    "prefix_prompt = f'{question} '\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for idx in range(len(word)):\n",
    "    char = word[idx]\n",
    "    \n",
    "    # encode context the generation is conditioned on\n",
    "    if len(outputs)==0:\n",
    "        input_sentence = f'{prefix_prompt}{\" \".join(outputs).strip()} {char}'\n",
    "    else:\n",
    "        input_sentence = f'{prefix_prompt} {\" \".join(outputs).strip()} {char}'\n",
    "    input_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)\n",
    "\n",
    "    \n",
    "    if idx == 0:\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            early_stopping=True,\n",
    "            num_beams=10, \n",
    "            no_repeat_ngram_size=2, \n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=0.5,\n",
    "            top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "        )\n",
    "    \n",
    "        \n",
    "    elif idx != len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2, \n",
    "            num_beams=10, \n",
    "            do_sample=False, # 샘플링 전략 사용\n",
    "            # temperature=0.25,\n",
    "            # top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "        )\n",
    "    \n",
    "    elif idx == len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            # max_new_tokens = 100,\n",
    "            max_length=len(input_ids[0]) + 100,\n",
    "            early_stopping=True,\n",
    "            num_beams=10, \n",
    "            no_repeat_ngram_size=2, \n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=1,\n",
    "            top_p=0.90, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "            \n",
    "        )\n",
    "\n",
    "        \n",
    "    output = tokenizer.decode(beam_output[0], skip_special_tokens=True)[len(input_sentence)-1:].split('\\n')[0].split('#')[0]\n",
    "    if '.' in output:\n",
    "        output = output.split('. ')[0] + '.'\n",
    "    # if ',' in output:\n",
    "        # output = output.split(', ')[0] + ','\n",
    "\n",
    "        \n",
    "    outputs.append(output)\n",
    "\n",
    "    print(f\"{char} : {outputs[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "202652c3-84ad-4920-a535-0efe598f3d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트 : 트와이스는 정말로 대단한 그룹이에요!\n",
      "와 : 와, 트와이이스의 노래는 항상 최고에요! 트위치에서 트위스트를 치면 트과이스가 춤을 추는 영상이 나와요!\n",
      "이 : 이 그룹은 정말 멋진 무대 퍼포먼스를 보여줘요.\n",
      "스 : 스윗한 미소와 귀여운 외모로 전 세계 팬들을 사로잡았어요..\n"
     ]
    }
   ],
   "source": [
    "word = \"트와이스\"\n",
    "question = f\"{word}에 대한 재미있고 간단한 문장을 말씀드리겠습니다.\"\n",
    "prefix_prompt = f'{question} '\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for idx in range(len(word)):\n",
    "    char = word[idx]\n",
    "    \n",
    "    # encode context the generation is conditioned on\n",
    "    if len(outputs)==0:\n",
    "        input_sentence = f'{prefix_prompt}{\" \".join(outputs).strip()} {char}'\n",
    "    else:\n",
    "        input_sentence = f'{prefix_prompt} {\" \".join(outputs).strip()} {char}'\n",
    "    input_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)\n",
    "\n",
    "    \n",
    "    if idx == 0:\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            early_stopping=True,\n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=0.5,\n",
    "        )\n",
    "    \n",
    "        \n",
    "    elif idx != len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2, \n",
    "            do_sample=False, # 샘플링 전략 사용\n",
    "            temperature=0.25,\n",
    "            # top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "        )\n",
    "    \n",
    "    elif idx == len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            # max_new_tokens = 100,\n",
    "            max_length=len(input_ids[0]) + 100,\n",
    "            early_stopping=True,\n",
    "            num_beams=10, \n",
    "            no_repeat_ngram_size=2, \n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=1,\n",
    "            top_p=0.90, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "            \n",
    "        )\n",
    "\n",
    "        \n",
    "    output = tokenizer.decode(beam_output[0], skip_special_tokens=True)[len(input_sentence)-1:].split('\\n')[0].split('#')[0]\n",
    "    if '.' in output:\n",
    "        output = output.split('. ')[0] + '.'\n",
    "    # if ',' in output:\n",
    "        # output = output.split(',')[0] + ','\n",
    "\n",
    "        \n",
    "    outputs.append(output)\n",
    "\n",
    "    print(f\"{char} : {outputs[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e320c649-4340-43d3-86fb-a45f6f53c1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
