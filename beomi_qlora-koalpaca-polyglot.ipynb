{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c97da895-8c36-4246-b2f6-f757fa63fe9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Oct 21 13:59:54 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    Off  | 00000000:01:00.0 Off |                  Off |\n",
      "| 30%   50C    P8    32W / 300W |      3MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    Off  | 00000000:05:00.0 Off |                  Off |\n",
      "| 59%   82C    P2   274W / 300W |  34105MiB / 48685MiB |     93%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    Off  | 00000000:06:00.0 Off |                  Off |\n",
      "| 30%   32C    P8    26W / 300W |   9640MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    Off  | 00000000:08:00.0 Off |                  Off |\n",
      "| 30%   33C    P8    22W / 300W |  15042MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA RTX A6000    Off  | 00000000:0E:00.0 Off |                  Off |\n",
      "| 30%   34C    P8    22W / 300W |      0MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA RTX A6000    Off  | 00000000:0F:00.0 Off |                  Off |\n",
      "| 30%   34C    P8    25W / 300W |  26153MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca20e034-6024-43b5-a29f-275e69b242b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f69c091-0d21-493c-9957-35f328fae708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q bitsandbytes\n",
    "# !pip install -q transformers \n",
    "# !pip install -q peft\n",
    "# !pip install -q accelerate\n",
    "# !pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312da2b5-1da0-4357-99ec-99b9239cf7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07105266-8f81-45ad-85f1-e6d77f23c4e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b9db9d9ec54e3f8f4a31d22cfc405e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPTNeoXForCausalLM(\n",
       "      (gpt_neox): GPTNeoXModel(\n",
       "        (embed_in): Embedding(30080, 5120)\n",
       "        (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (layers): ModuleList(\n",
       "          (0-39): 40 x GPTNeoXLayer(\n",
       "            (input_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_layernorm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "            (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (attention): GPTNeoXAttention(\n",
       "              (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "              (query_key_value): Linear4bit(\n",
       "                in_features=5120, out_features=15360, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=15360, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (mlp): GPTNeoXMLP(\n",
       "              (dense_h_to_4h): Linear4bit(in_features=5120, out_features=20480, bias=True)\n",
       "              (dense_4h_to_h): Linear4bit(in_features=20480, out_features=5120, bias=True)\n",
       "              (act): GELUActivation()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layer_norm): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (embed_out): Linear(in_features=5120, out_features=30080, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model_id = \"beomi/qlora-koalpaca-polyglot-12.8b-50step\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, quantization_config=bnb_config, device_map={\"\":0}, pad_token_id=tokenizer.eos_token_id)\n",
    "model = PeftModel.from_pretrained(model, peft_model_id).to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ecf8ef65-ece9-4508-a4f8-e06b2c2b1a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자 : 자, 이제 내가 생각하는 그는,\n",
      "전 : 전세계적으로 유명한, 그의 이름은,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "거 : 거시기다.\n"
     ]
    }
   ],
   "source": [
    "question = \"간단한 문장을 제시하라.\"\n",
    "word = \"자전거\"\n",
    "prefix_prompt = f'### 입력: {question}\\n\\n### 답변:'\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for idx in range(len(word)):\n",
    "    char = word[idx]\n",
    "    \n",
    "    # encode context the generation is conditioned on\n",
    "    if len(outputs)==0:\n",
    "        input_sentence = f'{prefix_prompt}{\" \".join(outputs).strip()} {char}'\n",
    "    else:\n",
    "        input_sentence = f'{prefix_prompt} {\" \".join(outputs).strip()} {char}'\n",
    "    input_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)\n",
    "\n",
    "    \n",
    "    if idx == 0:\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_length=len(input_ids[0]) + 10,\n",
    "            early_stopping=True,\n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=1.25,\n",
    "        )\n",
    "    \n",
    "        \n",
    "    elif idx != len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            # max_length=len(input_ids[0]) + 10,\n",
    "            early_stopping=True,\n",
    "            # num_beams=10, \n",
    "            # no_repeat_ngram_size=2, \n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=0.5,\n",
    "            # top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "        )\n",
    "    \n",
    "    elif idx == len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            # max_length=len(input_ids[0]) + 10,\n",
    "            early_stopping=True,\n",
    "            # num_beams=10, \n",
    "            # no_repeat_ngram_size=2, \n",
    "            do_sample=False, # 샘플링 전략 사용\n",
    "            # top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "            \n",
    "            \n",
    "            # input_ids = input_ids, \n",
    "            # max_length=len(input_ids[0]) + 50,\n",
    "            # early_stopping=True,\n",
    "            # num_beams=10, \n",
    "            no_repeat_ngram_size=2, \n",
    "            # do_sample=True, # 샘플링 전략 사용\n",
    "            # top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(beam_output[0], skip_special_tokens=True)[len(input_sentence)-1:].split('#')[0].split('. ')[0].strip() # .split('\\n')[0]\n",
    "    outputs.append(output)\n",
    "\n",
    "    print(f\"{char} : {outputs[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2504b139-bab1-4975-8c33-b25beaae153d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인 : 인서울대학이라는 말이 있습니다.\n",
      "하 : 하물며 인하대는 인인천대학입니다.\n",
      "대 : 대기업에 취업하는 학생들의 출신대학을 보면 인하대가 아주 높은 위치에 있다는 것을 확인하실 수 있으실 겁니다.\n"
     ]
    }
   ],
   "source": [
    "word = \"인하대\"\n",
    "question = f\"{word}에 대한 재미있고 간단한 문장을 말씀드리겠습니다.\"\n",
    "prefix_prompt = f'{question} '\n",
    "\n",
    "\n",
    "outputs = []\n",
    "for idx in range(len(word)):\n",
    "    char = word[idx]\n",
    "    \n",
    "    # encode context the generation is conditioned on\n",
    "    if len(outputs)==0:\n",
    "        input_sentence = f'{prefix_prompt}{\" \".join(outputs).strip()} {char}'\n",
    "    else:\n",
    "        input_sentence = f'{prefix_prompt} {\" \".join(outputs).strip()} {char}'\n",
    "    input_ids = tokenizer.encode(input_sentence, return_tensors='pt').to(device)\n",
    "\n",
    "    \n",
    "    if idx == 0:\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            early_stopping=True,\n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=0.5,\n",
    "        )\n",
    "    \n",
    "        \n",
    "    elif idx != len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            max_new_tokens = 100,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2, \n",
    "            do_sample=False, # 샘플링 전략 사용\n",
    "            temperature=0.25,\n",
    "            # top_p=0.95, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "        )\n",
    "    \n",
    "    elif idx == len(word)-1:\n",
    "        # set no_repeat_ngram_size to 2\n",
    "        beam_output = model.generate(\n",
    "            input_ids = input_ids, \n",
    "            # max_new_tokens = 100,\n",
    "            max_length=len(input_ids[0]) + 100,\n",
    "            early_stopping=True,\n",
    "            num_beams=10, \n",
    "            no_repeat_ngram_size=2, \n",
    "            do_sample=True, # 샘플링 전략 사용\n",
    "            temperature=1,\n",
    "            top_p=0.90, # 누적 확률이 95%인 후보집합에서만 생성\n",
    "            # top_k=50, # 확률 순위가 50위 밖인 토큰은 샘플링에서 제외\n",
    "            \n",
    "        )\n",
    "\n",
    "        \n",
    "    output = tokenizer.decode(beam_output[0], skip_special_tokens=True)[len(input_sentence)-1:].split('\\n')[0].split('#')[0]\n",
    "    if '.' in output:\n",
    "        output = output.split('.')[0] + '.'\n",
    "    if ',' in output:\n",
    "        output = output.split(',')[0] + ','\n",
    "\n",
    "        \n",
    "    outputs.append(output)\n",
    "\n",
    "    print(f\"{char} : {outputs[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8204c-d5a5-435d-8b59-bf0d41d628ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
